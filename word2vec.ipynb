{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     D:\\Users\\maraujo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     D:\\Users\\maraujo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     D:\\Users\\maraujo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from random import choice, random, randint\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from unicodedata import normalize\n",
    "\n",
    "nltk.download('rslp')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparando dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listando arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentencas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Queremos viralizar': a realidade por trás das...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recém-casados em algumas partes da Índia estão...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nayana, de 23 anos, é enfermeira. Abhijith, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“Queríamos que viralizasse”, diz Nayana. Ela a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nayana diz que acompanha as fotos de noivos in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentencas\n",
       "0  'Queremos viralizar': a realidade por trás das...\n",
       "1  Recém-casados em algumas partes da Índia estão...\n",
       "2  Nayana, de 23 anos, é enfermeira. Abhijith, de...\n",
       "3  “Queríamos que viralizasse”, diz Nayana. Ela a...\n",
       "4  Nayana diz que acompanha as fotos de noivos in..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_dir = './artigos'\n",
    "texts_list = [f'{texts_dir}/{file}' for file in  os.listdir(texts_dir)]\n",
    "\n",
    "todas_sentencas = []\n",
    "for text in texts_list:\n",
    "    arquivo = open(text, 'r', encoding='utf-8')\n",
    "    sentencas = arquivo.readlines()\n",
    "    arquivo.close()\n",
    "\n",
    "    todas_sentencas.append(sentencas)\n",
    "\n",
    "todas_sentencas = np.array(todas_sentencas, dtype=np.object)\n",
    "todas_sentencas = np.hstack(todas_sentencas)\n",
    "\n",
    "df = pd.DataFrame(todas_sentencas, columns=['sentencas'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentencas</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31946</th>\n",
       "      <td>O problema é que, no Enem 2020, por conta da p...</td>\n",
       "      <td>[o, problema, e, que, no, enem, por, conta, da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88506</th>\n",
       "      <td>\"Em que você está gastando dinheiro? Dar crédi...</td>\n",
       "      <td>[em, que, voce, esta, gastando, dinheiro, dar,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22291</th>\n",
       "      <td>Nessa experiência de um século, a BCG é aplica...</td>\n",
       "      <td>[nessa, experiencia, de, um, seculo, a, bcg, e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentencas  \\\n",
       "31946  O problema é que, no Enem 2020, por conta da p...   \n",
       "88506  \"Em que você está gastando dinheiro? Dar crédi...   \n",
       "22291  Nessa experiência de um século, a BCG é aplica...   \n",
       "\n",
       "                                                  tokens  \n",
       "31946  [o, problema, e, que, no, enem, por, conta, da...  \n",
       "88506  [em, que, voce, esta, gastando, dinheiro, dar,...  \n",
       "22291  [nessa, experiencia, de, um, seculo, a, bcg, e...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.RSLPStemmer() # Objeto para \"stemização\" em português\n",
    "\n",
    "def remover_acentos(frase):\n",
    "    return normalize('NFKD', frase).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "RETIRAR_STOP_WORDS = False\n",
    "ESTEMIZAR = False\n",
    "\n",
    "if (RETIRAR_STOP_WORDS):\n",
    "    stopwords = [remover_acentos(sw) for sw in  nltk.corpus.stopwords.words('portuguese')]\n",
    "else:\n",
    "    stopwords = []\n",
    "\n",
    "def normalizar_frase(frase):\n",
    "    frase = str(frase).lower()\n",
    "    frase = remover_acentos(frase)\n",
    "    frase = re.sub(r'\\w*[0-9]+\\w*', '', frase)\n",
    "    frase = re.sub(r'[\\[\\]º!\"#$%&\\'()*+,-./:;<=>?@^_`~]+', '', frase) # remove caracteres especiais\n",
    "    \n",
    "    tokens = [stemmer.stem(tk) if ESTEMIZAR else tk for tk in word_tokenize(frase) if tk not in stopwords] # removendo stopwords e estemizando\n",
    "\n",
    "    return tokens\n",
    "\n",
    "df['tokens'] = df['sentencas'].apply(normalizar_frase)\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando Vocabulário e Dicionário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for tokens in df['tokens'].values:\n",
    "    counter.update(tokens)\n",
    "\n",
    "FREQUENCIA_MINIMA = 2\n",
    "vocab_words = [tk for tk,freq in list(counter.items()) if freq >= FREQUENCIA_MINIMA]\n",
    "words2idx = {w:n for n,w in enumerate(vocab_words)}\n",
    "idx2words = {n:w for n,w in enumerate(vocab_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refatorando novamente a lista\n",
    "- Neste passo são removidos os tokens que baixa frequência além de transformá-los em seus índices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 136040/136040 [14:10<00:00, 159.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Limpando as sentenças novamente agora só com as palavras oficiais\n",
    "sentencas_final = []\n",
    "for sentenca in tqdm(df['tokens'].values, ncols=100):\n",
    "    vetor = [words2idx[w] for w in sentenca if w in vocab_words]\n",
    "    sentencas_final.append(vetor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o dataset\n",
    "- Agora criamos o dataset (de acordo com seus índices) utilizando o algoritmo [CBOW Context Bag of Words](https://en.wikipedia.org/wiki/Word2vec#CBOW_and_skip_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 136040/136040 [00:06<00:00, 22280.50it/s]\n"
     ]
    }
   ],
   "source": [
    "class custom_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, n_window, lista_sentencas):\n",
    "        self.lista_sentencas = lista_sentencas\n",
    "        self.tensor_x, self.tensor_y = [], []\n",
    "        for sentenca in tqdm(self.lista_sentencas, ncols=100):\n",
    "            for ind_tk in range(len(sentenca)-2*n_window):\n",
    "                vetorx = sentenca[ind_tk:ind_tk+2*n_window+1]\n",
    "                vetory = vetorx.pop(n_window)\n",
    "                self.tensor_x.append(vetorx)\n",
    "                self.tensor_y.append(vetory)\n",
    "        \n",
    "        self.tensor_x = torch.tensor(self.tensor_x, dtype=torch.long)\n",
    "        self.tensor_y = torch.tensor(self.tensor_y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.tensor_x[index], self.tensor_y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tensor_x)\n",
    "\n",
    "dataset = custom_dataset(n_window=2, lista_sentencas=sentencas_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação do modelo\n",
    "- Vamos criar o modelo de rede neural bem simples, apenas utilizando camada linear e treinar de acordo com nosso dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Modelo_CBOW(\n",
       "  (embedding): Embedding(53278, 32, max_norm=1)\n",
       "  (linear): Sequential(\n",
       "    (0): ReLU()\n",
       "    (1): Linear(in_features=32, out_features=53278, bias=True)\n",
       "    (2): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Modelo_CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, vocab_size):\n",
    "        super(Modelo_CBOW, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, max_norm=1)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=embedding_dim, out_features=vocab_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.mean(axis=1)\n",
    "        return self.linear(x)\n",
    "\n",
    "model = Modelo_CBOW(embedding_dim=32, vocab_size=len(vocab_words))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 32\n",
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = Modelo_CBOW(embedding_dim=EMBEDDING_DIM, vocab_size=len(vocab_words))\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    loss_epoch = 0\n",
    "    for x, y_real in tqdm(dataloader, ncols=50):\n",
    "        x, y_real = x.to(DEVICE), y_real.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, y_real)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += loss.item()\n",
    "    \n",
    "    print (f'Epoch: {epoch:02} train_loss: {round(loss_epoch,3)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fdeb772b58d3bef31cdf07dec3c4d07d492c459ba8661e2ccd1805246a798c26"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
